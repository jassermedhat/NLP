{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d71ea1-43b1-4bd9-a523-d1ac33f9f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ex1.pdf\n",
      "✅ Found 9 sections.\n",
      "Structured JSON saved to 'structured_output.json'\n",
      "\n",
      "--- Sample output (first 3 sections) ---\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"header\": \"Software Engineering Section 1\",\n",
      "    \"content\": \"Difference between Software and Computer programs Software engineering is intended to support professional software development, rather than individual programming. It includes techniques that support program specification, design, and evolution, none of which are normally relevant for personal software development. Many people think that software is simply another word for computer programs. However, when we are talking about software engineering, software is not just the programs themselves but also all associated documentation and configuration data that is required to make these programs operate correctly. A professionally developed software system is often more than a single program. The system usually consists of a number of separate programs and configuration files that are used to set up these programs. It may include system documentation, which describes the structure of the system; user documentation, which explains how to use the system, and websites for users to download recent product information. This is one of the important differences between professional and amateur software development. If you are writing a program for yourself, no one else will use it and you don’t have to worry about writing program guides, documenting the program design, etc. However, if you are writing software that other people will use and other engineers will change then you usually have to provide additional information as well as the code of the program. Many applications need software engineering; they do not all need the same software engineering techniques. There are still many reports of software projects going wrong and ‘software failures’. Software engineering is criticized as inadequate for modern software development. However, in my view, many of these so-called software failures are a\"\n",
      "  },\n",
      "  {\n",
      "    \"header\": \"consequence of two factors\",\n",
      "    \"content\": \"​ Increasing demands ​ Low expectations 1 When we talk about the quality of professional software, we have to take into account that the software is used and changed by people apart from its developers. Quality is therefore not just concerned with what the software does. Rather, it has to include the software’s behavior while it is executing. This is reflected in so-called quality or non-functional software attributes. Examples of these attributes are the software’s response time to a user query and the understandability of the program code. The specific set of attributes that you might expect from a software system obviously depends on its application. Therefore, a banking system must be secure, an interactive game must be responsive, a telephone switching system must be reliable, and so on. Essential attributes of a good software Maintainability Software should be written in such a way so that it can evolve to meet the changing needs of customers. This is a critical attribute because software change is an inevitable requirement of a changing business environment. Dependability and security Software dependability includes a range of characteristics including reliability, security, and safety. Dependable software should not cause physical or economic damage in the event of system failure. Malicious users should not be able to access or damage the system. Efficiency Software should not make wasteful use of system resources such as memory and processor cycles. Efficiency therefore includes responsiveness, processing time, memory utilization, etc. Acceptability Software must be acceptable to the type of users for which it is designed. This means that it must be understandable, usable, and compatible with other systems that they use. 2\"\n",
      "  },\n",
      "  {\n",
      "    \"header\": \"SFW Engineering\",\n",
      "    \"content\": \"Software engineering is an engineering discipline that is concerned with all aspects of software production from the early stages of system specification through to maintaining the system after it has gone into use.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#############   USE   THIS ONE  FOR STEP 1\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        return file.read()\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    return '\\n'.join(para.text.strip() for para in doc.paragraphs if para.text.strip())\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    text = ''\n",
    "    try:\n",
    "        with fitz.open(path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to read PDF '{path}': {e}\")\n",
    "    return text\n",
    "\n",
    "def load_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    loaders = {'.pdf': extract_text_from_pdf, '.docx': extract_text_from_docx, '.txt': extract_text_from_txt}\n",
    "    if ext not in loaders:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "    return loaders[ext](file_path)\n",
    "\n",
    "def clean_raw_text(raw_text):\n",
    "    # Basic cleaning: remove page numbers, bullets, weird characters, multiple spaces, multiple newlines\n",
    "    text = re.sub(r'\\n\\d+\\n', '\\n', raw_text)  # Remove isolated page numbers\n",
    "    text = re.sub(r'[•●▪■\\u2022\\uf0b7]', '', text)  # Remove common bullets\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Replace multiple spaces/tabs with single space\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Limit newlines to max two\n",
    "\n",
    "    # Remove obvious OCR garbage or corrupted words (customize as needed)\n",
    "    text = re.sub(r'\\bCo\\s*i\\s*ant\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]+', '', text)  # Remove control chars\n",
    "\n",
    "    # Strip trailing/leading spaces on each line\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def is_probable_header(line):\n",
    "    line = line.strip()\n",
    "    # Exclude numbered or lettered list items like '1) ...', 'a) ...', '2. ...'\n",
    "    if re.match(r'^(\\d+[\\.\\)]|[a-zA-Z][\\.\\)])\\s', line):\n",
    "        return False\n",
    "\n",
    "    # Ends with colon or dash\n",
    "    if re.search(r'[:\\-]\\s*$', line):\n",
    "        return True\n",
    "\n",
    "    # Mostly capitalized words, short line\n",
    "    tokens = line.split()\n",
    "    if 1 <= len(tokens) <= 15:\n",
    "        upper_words = sum(1 for t in tokens if t and t[0].isupper())\n",
    "        if upper_words / len(tokens) > 0.7:\n",
    "            return True\n",
    "\n",
    "    # Fully uppercase (ignore short lines)\n",
    "    if len(line) > 3 and line.isupper():\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def structure_text_to_sections(text):\n",
    "    lines = text.splitlines()\n",
    "    sections = []\n",
    "    current_header = None\n",
    "    current_content = []\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        if not line.strip():\n",
    "            # Empty line, consider as paragraph break - add current content if any\n",
    "            if current_content:\n",
    "                # Append paragraph (join with space)\n",
    "                current_content.append('')  # Add paragraph break as empty string\n",
    "            continue\n",
    "\n",
    "        if is_probable_header(line):\n",
    "            # Save previous section if exists\n",
    "            if current_header or current_content:\n",
    "                content_text = \" \".join(p for p in current_content if p).strip()\n",
    "                if current_header is None:\n",
    "                    # No header before content? Use default header\n",
    "                    current_header = \"Document\"\n",
    "                sections.append({\n",
    "                    \"header\": current_header,\n",
    "                    \"content\": content_text\n",
    "                })\n",
    "                current_content = []\n",
    "            current_header = line.rstrip(':-').strip()\n",
    "        else:\n",
    "            current_content.append(line.strip())\n",
    "\n",
    "    # Add last section\n",
    "    if current_header or current_content:\n",
    "        content_text = \" \".join(p for p in current_content if p).strip()\n",
    "        if current_header is None:\n",
    "            current_header = \"Document\"\n",
    "        sections.append({\n",
    "            \"header\": current_header,\n",
    "            \"content\": content_text\n",
    "        })\n",
    "\n",
    "    # Remove empty content sections if any\n",
    "    sections = [s for s in sections if s['content'].strip() != '']\n",
    "\n",
    "    return sections\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def main(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File '{filepath}' does not exist.\")\n",
    "\n",
    "    print(f\"Processing file: {filepath}\")\n",
    "\n",
    "    raw_text = load_text(filepath)\n",
    "    cleaned_text = clean_raw_text(raw_text)\n",
    "    sections = structure_text_to_sections(cleaned_text)\n",
    "\n",
    "    if not sections:\n",
    "        print(\"⚠️ Warning: No sections found after processing.\")\n",
    "    else:\n",
    "        print(f\"✅ Found {len(sections)} sections.\")\n",
    "\n",
    "    # Save to JSON\n",
    "    save_json(sections, \"structured_output.json\")\n",
    "    print(\"Structured JSON saved to 'structured_output.json'\")\n",
    "\n",
    "    # Print first 3 sections as preview\n",
    "    print(\"\\n--- Sample output (first 3 sections) ---\\n\")\n",
    "    print(json.dumps(sections[:3], ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"ex1.pdf\")  # Change to your file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206c692f-3577-445b-9823-c71a84d91bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RusRus\\anaconda3\\envs\\dgnbf\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 76 sentences, filtered to 21 by relevance >= 0.5\n",
      "Output saved to 'processed_stage2.json'\n"
     ]
    }
   ],
   "source": [
    "###################### USE THIS ONE FOR STEP 2\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load models once\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "kw_model = KeyBERT()\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    tokenizer=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Remove leading numbering or bullets in sentences and newlines inside\n",
    "    sentence = re.sub(r'^[\\d\\)\\.\\-\\s]+', '', sentence)\n",
    "    sentence = sentence.replace('\\n', ' ').strip()\n",
    "    return sentence\n",
    "\n",
    "def segment_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 15]\n",
    "\n",
    "def extract_keywords(sentence, top_n=5):\n",
    "    kws = kw_model.extract_keywords(sentence, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=top_n)\n",
    "    return [kw[0] for kw in kws]\n",
    "\n",
    "def extract_entities(sentence):\n",
    "    entities = ner_pipeline(sentence)\n",
    "    # Aggregate unique entity texts, filter punctuation and spaces\n",
    "    unique_entities = list({ent['word'].strip() for ent in entities if ent['word'].strip() and not ent['word'].isspace()})\n",
    "    return unique_entities\n",
    "\n",
    "def compute_relevance(sentence, context_emb):\n",
    "    sent_emb = sentence_model.encode(sentence, convert_to_tensor=True)\n",
    "    score = util.pytorch_cos_sim(sent_emb, context_emb).item()\n",
    "    return round(score, 4)\n",
    "\n",
    "def process_section(section, global_context_emb):\n",
    "    \"\"\"\n",
    "    Process a single section {header, content}:\n",
    "    - Segment content to sentences\n",
    "    - For each sentence, extract keywords, entities, compute relevance vs global context\n",
    "    \"\"\"\n",
    "    header = section.get('header', '').strip()\n",
    "    content = section.get('content', '').strip()\n",
    "    combined_text = f\"{header}. {content}\"  # Use header+content as local context if needed\n",
    "    \n",
    "    # Compute local context embedding for relevance comparison (optional: could use global context too)\n",
    "    local_context_emb = sentence_model.encode(combined_text, convert_to_tensor=True)\n",
    "    \n",
    "    sentences = segment_sentences(content)\n",
    "    results = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        clean_sent = clean_sentence(sent)\n",
    "        keywords = extract_keywords(clean_sent)\n",
    "        entities = extract_entities(clean_sent)\n",
    "        answer_candidates = entities if entities else keywords\n",
    "        relevance_score = compute_relevance(clean_sent, global_context_emb)\n",
    "        \n",
    "        results.append({\n",
    "            \"header\": header,\n",
    "            \"sentence_num\": i + 1,\n",
    "            \"sentence\": sent,\n",
    "            \"clean_sentence\": clean_sent,\n",
    "            \"keywords\": keywords,\n",
    "            \"entities\": entities,\n",
    "            \"answer_candidates\": answer_candidates,\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def load_structured_json(path=\"structured_output.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def build_global_context(sections, max_chars=2048):\n",
    "    # Combine headers + contents to build a global context string for relevance\n",
    "    combined_text = \" \".join(f\"{sec.get('header','')} {sec.get('content','')}\" for sec in sections)\n",
    "    return combined_text[:max_chars]\n",
    "\n",
    "def main():\n",
    "    structured_data = load_structured_json(\"structured_output.json\")\n",
    "    global_context_text = build_global_context(structured_data)\n",
    "    global_context_emb = sentence_model.encode(global_context_text, convert_to_tensor=True)\n",
    "\n",
    "    all_results = []\n",
    "    for section in structured_data:\n",
    "        section_results = process_section(section, global_context_emb)\n",
    "        all_results.extend(section_results)\n",
    "\n",
    "    # Optionally filter by relevance threshold (e.g., 0.5)\n",
    "    filtered_results = [r for r in all_results if r[\"relevance_score\"] >= 0.5]\n",
    "\n",
    "    # Save output for MCQ generation\n",
    "    with open(\"processed_stage2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filtered_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Processed {len(all_results)} sentences, filtered to {len(filtered_results)} by relevance >= 0.5\")\n",
    "    print(\"Output saved to 'processed_stage2.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e7d340-c5c5-403f-a816-b5377c8520a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RusRus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\RusRus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCQ generation complete. Generated 7 MCQs, skipped 11 entries due to quality/filtering.\n",
      "Output saved to: C:\\Users\\RusRus\\improved_mcqs.json\n"
     ]
    }
   ],
   "source": [
    "######################## USE THIS ONE FOR STEP 3\n",
    "import json\n",
    "import random\n",
    "from typing import List\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import wordnet\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Ensure nltk data is downloaded\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load the QG model with proper task\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qg-hl\")\n",
    "\n",
    "# Load embedding model for distractors\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Input JSON\n",
    "input_file = r\"C:\\Users\\RusRus\\processed_stage2.json\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "def generate_question_qg(context: str, answer: str) -> str:\n",
    "    \"\"\"Generate a question using valhalla/t5-small-qg-hl with <hl> highlighting.\"\"\"\n",
    "    if answer not in context:\n",
    "        return \"\"  # Avoid incorrect highlighting\n",
    "    highlighted = context.replace(answer, f\"<hl> {answer} <hl>\")\n",
    "    prompt = f\"generate question: {highlighted}\"\n",
    "    try:\n",
    "        result = qg_pipeline(prompt, max_length=64, do_sample=False)\n",
    "        return result[0][\"generated_text\"].strip() if result else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"QG error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_wordnet_distractors(word: str) -> List[str]:\n",
    "    \"\"\"Generate distractors using WordNet.\"\"\"\n",
    "    distractors = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            name = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if name != word.lower():\n",
    "                distractors.add(name)\n",
    "    return list(distractors)[:5]\n",
    "\n",
    "\n",
    "def get_embedding_distractors(correct_answer: str, context: str, top_k=5) -> List[str]:\n",
    "    \"\"\"Find better distractors using sentence embeddings and word filtering.\"\"\"\n",
    "    words = list(set(context.lower().split()))\n",
    "    words = [w.strip(\".,()[]\") for w in words if len(w) > 3 and w.lower() != correct_answer.lower()]\n",
    "    words = [w for w in words if w.isalpha()]  # Remove punctuation, numbers, etc.\n",
    "\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        correct_embedding = embedding_model.encode(correct_answer, convert_to_tensor=True)\n",
    "        candidate_embeddings = embedding_model.encode(words, convert_to_tensor=True)\n",
    "        similarities = util.pytorch_cos_sim(correct_embedding, candidate_embeddings)[0]\n",
    "        sorted_indices = similarities.argsort(descending=True)  # Get most similar\n",
    "        distractors = []\n",
    "        for idx in sorted_indices:\n",
    "            candidate = words[idx]\n",
    "            if candidate.lower() != correct_answer.lower() and candidate not in distractors:\n",
    "                distractors.append(candidate)\n",
    "            if len(distractors) == top_k:\n",
    "                break\n",
    "        return distractors\n",
    "    except Exception as e:\n",
    "        print(f\"Distractor generation error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def clean_question(question: str, answer: str) -> str:\n",
    "    \"\"\"Ensure the answer is not directly embedded in the question.\"\"\"\n",
    "    q_lower = question.lower()\n",
    "    a_lower = answer.lower()\n",
    "    if a_lower in q_lower:\n",
    "        return question.replace(answer, \"_____\").strip()\n",
    "    return question.strip()\n",
    "\n",
    "\n",
    "def replace_abbreviation_in_question(question: str, answer: str, abbreviation_candidates: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Replace any detected abbreviation or acronym in the question with the full answer.\n",
    "\n",
    "    abbreviation_candidates: list of abbreviations/acronyms extracted from the sentence that relate to the answer.\n",
    "    \"\"\"\n",
    "    # To avoid partial replacement, do whole word match with regex\n",
    "    for abbr in abbreviation_candidates:\n",
    "        pattern = re.compile(r'\\b' + re.escape(abbr) + r'\\b', flags=re.IGNORECASE)\n",
    "        if pattern.search(question):\n",
    "            question = pattern.sub(answer, question)\n",
    "    return question\n",
    "\n",
    "\n",
    "def find_abbreviations(sentence: str, answer: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Heuristic: find abbreviations in sentence inside parentheses next to answer.\n",
    "\n",
    "    Example: \"Network Interface Card (NIC)\" → extract \"NIC\"\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(answer)}\\s*\\(([^)]+)\\)\", flags=re.IGNORECASE)\n",
    "    match = pattern.search(sentence)\n",
    "    if match:\n",
    "        # Return all uppercase abbreviations or short acronyms split by comma/semicolon if any\n",
    "        abbrs = [abbr.strip() for abbr in re.split(r'[;,]', match.group(1))]\n",
    "        # Filter to plausible abbreviations (usually uppercase or short)\n",
    "        return [a for a in abbrs if len(a) <= 6 and a.isupper()]\n",
    "    return []\n",
    "\n",
    "\n",
    "# Generate MCQs\n",
    "output_mcqs = []\n",
    "mcq_target = 7  # Number of MCQs desired\n",
    "skipped_entries = 0\n",
    "\n",
    "for entry in data:\n",
    "    if len(output_mcqs) >= mcq_target:\n",
    "        break\n",
    "\n",
    "    sentence = entry.get(\"clean_sentence\", \"\").strip()\n",
    "    answer = entry.get(\"answer_candidates\", [])[0] if entry.get(\"answer_candidates\") else \"\"\n",
    "\n",
    "    if not sentence or not answer:\n",
    "        skipped_entries += 1\n",
    "        continue\n",
    "\n",
    "    question = generate_question_qg(sentence, answer)\n",
    "    if not question:\n",
    "        skipped_entries += 1\n",
    "        continue\n",
    "\n",
    "    # Detect abbreviations related to answer\n",
    "    abbreviations = find_abbreviations(sentence, answer)\n",
    "\n",
    "    # Replace abbreviation in question with full answer text\n",
    "    if abbreviations:\n",
    "        question = replace_abbreviation_in_question(question, answer, abbreviations)\n",
    "\n",
    "    question = clean_question(question, answer)\n",
    "\n",
    "    # Relaxed filtering: min question length 3 words, allow no question mark (some models omit)\n",
    "    if len(question.split()) < 3:\n",
    "        skipped_entries += 1\n",
    "        continue\n",
    "\n",
    "    if answer.lower() in question.lower():\n",
    "        # If answer still leaks into question, skip\n",
    "        skipped_entries += 1\n",
    "        continue\n",
    "\n",
    "    # Get distractors\n",
    "    distractors = get_embedding_distractors(answer, sentence)\n",
    "    if len(distractors) < 3:\n",
    "        distractors += get_wordnet_distractors(answer)\n",
    "    distractors = list(set(distractors))[:3]\n",
    "\n",
    "    if len(distractors) < 3:\n",
    "        skipped_entries += 1\n",
    "        continue\n",
    "\n",
    "    options = distractors + [answer]\n",
    "    random.shuffle(options)\n",
    "\n",
    "    output_mcqs.append({\n",
    "        \"header\": entry.get(\"header\", \"General\"),\n",
    "        \"question\": question,\n",
    "        \"options\": options,\n",
    "        \"answer\": answer,\n",
    "        \"source_sentence\": sentence\n",
    "    })\n",
    "\n",
    "# Save output\n",
    "output_file = r\"C:\\Users\\RusRus\\improved_mcqs.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_mcqs, f, indent=2)\n",
    "\n",
    "print(f\"✅ MCQ generation complete. Generated {len(output_mcqs)} MCQs, skipped {skipped_entries} entries due to quality/filtering.\")\n",
    "print(f\"Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd7634-3af0-49ae-bab9-2f5d38464dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RusRus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\RusRus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCQ generation complete. Output saved to: C:\\Users\\RusRus\\improved_mcqs.json\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION FOR STEP 3 , MORE STRICT FILTERING\n",
    "# import json\n",
    "# import random\n",
    "# from typing import List\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# from nltk.corpus import wordnet\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import nltk\n",
    "\n",
    "# # Ensure nltk data is downloaded\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "\n",
    "# # Load the QG model with proper task\n",
    "# qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qg-hl\")\n",
    "\n",
    "# # Load embedding model for distractors\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Input JSON\n",
    "# input_file = r\"C:\\Users\\RusRus\\processed_stage2.json\"\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# def generate_question_qg(context: str, answer: str) -> str:\n",
    "#     \"\"\"Generate a question using valhalla/t5-small-qg-hl with <hl> highlighting.\"\"\"\n",
    "#     if answer not in context:\n",
    "#         return \"\"  # Avoid incorrect highlighting\n",
    "#     highlighted = context.replace(answer, f\"<hl> {answer} <hl>\")\n",
    "#     prompt = f\"generate question: {highlighted}\"\n",
    "#     try:\n",
    "#         result = qg_pipeline(prompt, max_length=64, do_sample=False)\n",
    "#         return result[0][\"generated_text\"].strip() if result else \"\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"QG error: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def get_wordnet_distractors(word: str) -> List[str]:\n",
    "#     \"\"\"Generate distractors using WordNet\"\"\"\n",
    "#     distractors = set()\n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             name = lemma.name().replace(\"_\", \" \").lower()\n",
    "#             if name != word.lower():\n",
    "#                 distractors.add(name)\n",
    "#     return list(distractors)[:5]\n",
    "\n",
    "# def get_embedding_distractors(correct_answer: str, context: str, top_k=5) -> List[str]:\n",
    "#     \"\"\"Find better distractors using sentence embeddings and word filtering.\"\"\"\n",
    "#     words = list(set(context.lower().split()))\n",
    "#     words = [w.strip(\".,()[]\") for w in words if len(w) > 3 and w.lower() != correct_answer.lower()]\n",
    "#     words = [w for w in words if w.isalpha()]  # Remove punctuation, numbers, etc.\n",
    "\n",
    "#     if not words:\n",
    "#         return []\n",
    "\n",
    "#     try:\n",
    "#         correct_embedding = embedding_model.encode(correct_answer, convert_to_tensor=True)\n",
    "#         candidate_embeddings = embedding_model.encode(words, convert_to_tensor=True)\n",
    "#         similarities = util.pytorch_cos_sim(correct_embedding, candidate_embeddings)[0]\n",
    "#         sorted_indices = similarities.argsort(descending=True)  # Get most similar, not least\n",
    "#         distractors = []\n",
    "#         for idx in sorted_indices:\n",
    "#             candidate = words[idx]\n",
    "#             if candidate.lower() != correct_answer.lower() and candidate not in distractors:\n",
    "#                 distractors.append(candidate)\n",
    "#             if len(distractors) == top_k:\n",
    "#                 break\n",
    "#         return distractors\n",
    "#     except Exception as e:\n",
    "#         print(f\"Distractor generation error: {e}\")\n",
    "#         return []\n",
    "\n",
    "\n",
    "# def clean_question(question: str, answer: str) -> str:\n",
    "#     \"\"\"Ensure the answer is not directly embedded in the question.\"\"\"\n",
    "#     q_lower = question.lower()\n",
    "#     a_lower = answer.lower()\n",
    "#     if a_lower in q_lower:\n",
    "#         return question.replace(answer, \"_____\").strip()\n",
    "#     return question.strip()\n",
    "\n",
    "# # Generate MCQs\n",
    "# output_mcqs = []\n",
    "# for entry in data:\n",
    "#     sentence = entry.get(\"clean_sentence\", \"\").strip()\n",
    "#     answer = entry.get(\"answer_candidates\", [])[0] if entry.get(\"answer_candidates\") else \"\"\n",
    "\n",
    "#     if not sentence or not answer:\n",
    "#         continue\n",
    "\n",
    "#     question = generate_question_qg(sentence, answer)\n",
    "#     if not question:\n",
    "#         continue\n",
    "\n",
    "#     question = clean_question(question, answer)\n",
    "#     if answer.lower() in question.lower():\n",
    "#         continue  # Skip if answer still leaks into question\n",
    "\n",
    "#     # Get distractors\n",
    "#     distractors = get_embedding_distractors(answer, sentence)\n",
    "#     if len(distractors) < 3:\n",
    "#         distractors += get_wordnet_distractors(answer)\n",
    "#     distractors = list(set(distractors))[:3]\n",
    "\n",
    "#     if len(distractors) < 3:\n",
    "#         continue\n",
    "\n",
    "#     options = distractors + [answer]\n",
    "#     random.shuffle(options)\n",
    "\n",
    "#     output_mcqs.append({\n",
    "#         \"header\": entry.get(\"header\", \"General\"),\n",
    "#         \"question\": question,\n",
    "#         \"options\": options,\n",
    "#         \"answer\": answer,\n",
    "#         \"source_sentence\": sentence\n",
    "#     })\n",
    "\n",
    "# # Save output\n",
    "# output_file = r\"C:\\Users\\RusRus\\improved_mcqs.json\"\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(output_mcqs, f, indent=2)\n",
    "\n",
    "# print(f\"✅ MCQ generation complete. Output saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
